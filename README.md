# AI Coder Extension

Расширение для VS Code, предназначенное для разработки расширений генерации кода с использованием LLM (Large Language Models).

## Описание

Это расширение предоставляет основу для создания инструментов генерации кода с помощью языковых моделей. В текущей версии реализован базовый UI с полем ввода и кнопкой запуска, а также архитектурный задел для будущей интеграции с различными LLM провайдерами.

## Возможности

- ✅ Webview панель с интерфейсом для ввода запросов
- ✅ Поле для ввода текста (промпта)
- ✅ Кнопка для запуска генерации
- ✅ **Настройки LLM с полным UI**
  - Выбор провайдера (OpenAI, Anthropic, Ollama, Custom)
  - Безопасное хранение API ключей через VS Code SecretStorage
  - Настройка модели, температуры, максимального количества токенов
  - Настройка таймаута и базового URL для кастомных провайдеров
- ✅ **Поддержка локальных моделей**
  - Полная интеграция с Ollama (llama2, codellama, mistral и др.)
  - Поддержка локальных API (LM Studio, локальные серверы)
  - Проверка доступности локальных серверов
  - Настройка URL для локальных серверов
- ✅ Архитектурная основа для интеграции LLM
- ✅ Структура для будущего расширения функциональности

## Архитектура

### Структура проекта

```
.
├── src/
│   ├── extension.ts          # Точка входа расширения
│   ├── webview/
│   │   └── panel.ts          # Управление Webview панелью
│   └── services/
│       └── llmService.ts     # Сервис для работы с LLM (задел на будущее)
├── media/
│   ├── main.js               # JavaScript для Webview
│   └── main.css              # Стили для Webview
├── package.json              # Конфигурация расширения
└── tsconfig.json             # Конфигурация TypeScript
```

### Основные компоненты

1. **Extension.ts** - Главный файл расширения, отвечает за активацию и регистрацию команд
2. **Panel.ts** - Управляет Webview панелью, обрабатывает сообщения между UI и расширением
3. **LLMService.ts** - Сервис для работы с LLM (в текущей версии содержит заглушку, готов для интеграции реальных провайдеров)

## Установка и разработка

### Требования

- Node.js (версия 16.x или выше)
- VS Code или Cursor
- TypeScript

### Установка зависимостей

```bash
npm install
```

### Компиляция

```bash
npm run compile
```

### Запуск в режиме разработки

1. Откройте проект в VS Code
2. Нажмите `F5` для запуска Extension Development Host
3. В новом окне VS Code выполните команду `AI Coder: Open AI Coder Panel` из Command Palette (`Ctrl+Shift+P` / `Cmd+Shift+P`)

### Сборка для публикации

```bash
npm run vscode:prepublish
```

## Использование

### Первый запуск

1. Откройте Command Palette (`Ctrl+Shift+P` / `Cmd+Shift+P`)
2. Выполните команду `AI Coder: Open AI Coder Panel`
3. Перейдите на вкладку **"Настройки"**
4. Настройте параметры LLM:
   - Выберите провайдер (OpenAI, Anthropic, Ollama или Custom)
   - Введите API ключ (сохраняется в безопасном хранилище VS Code)
   - Укажите модель (например: `gpt-4`, `gpt-3.5-turbo`, `claude-3-opus`)
   - Настройте температуру и другие параметры
   - Нажмите "Сохранить настройки"

### Генерация кода

1. Перейдите на вкладку **"Генерация"**
2. Введите ваш запрос в текстовое поле
3. Нажмите кнопку "Сгенерировать код"
4. Результат отобразится в секции результата

### Настройки

Все настройки можно изменить через:
- **UI в расширении**: Вкладка "Настройки" в панели
- **VS Code Settings**: `File > Preferences > Settings` (поиск "AI Coder")
- **settings.json**: Прямое редактирование конфигурации

## Будущее развитие

### Планируемые возможности

- [ ] Интеграция с OpenAI API
- [ ] Интеграция с Anthropic Claude API
- [ ] Поддержка локальных моделей (Ollama, LM Studio)
- [ ] Кэширование запросов
- [ ] Стриминг ответов в реальном времени
- [ ] Настройки через VS Code Settings
- [ ] Поддержка контекста из открытых файлов
- [ ] История запросов
- [ ] Экспорт сгенерированного кода в файлы

### Архитектурный задел

Проект спроектирован с учетом будущего расширения:

1. **LLMService** - Абстракция для работы с различными LLM провайдерами
2. **Интерфейс LLMProvider** - Позволяет легко добавлять новые провайдеры
3. **Конфигурация через Settings** - Готова структура для настроек через `settings.json`
4. **Модульная структура** - Легко добавлять новые компоненты и сервисы

## Конфигурация

### Через UI (рекомендуется)

Используйте вкладку "Настройки" в панели расширения для удобной настройки всех параметров.

### Через settings.json

Вы можете настроить параметры напрямую в `settings.json`:

```json
{
  "aiCoder.llm.provider": "openai",
  "aiCoder.llm.model": "gpt-4",
  "aiCoder.llm.temperature": 0.7,
  "aiCoder.llm.maxTokens": 2000,
  "aiCoder.llm.baseUrl": "",
  "aiCoder.llm.timeout": 30000
}
```

**Важно**: API ключ не хранится в `settings.json` по соображениям безопасности. Он сохраняется в VS Code SecretStorage через UI расширения.

### Доступные провайдеры

- **openai** - OpenAI (GPT-3.5, GPT-4) - требует API ключ
- **anthropic** - Anthropic Claude - требует API ключ
- **ollama** - Ollama (локальные модели) - работает без API ключа
  - Поддерживает модели: llama2, codellama, mistral, phi и другие
  - По умолчанию использует `http://localhost:11434`
  - Можно настроить другой URL через настройки
- **custom** - Кастомный провайдер (OpenAI-совместимый API)
  - Подходит для LM Studio, локальных серверов и других решений
  - Требует указания baseUrl (например: `http://localhost:1234/v1`)
  - API ключ обычно не требуется

### Параметры

- **provider** - Провайдер LLM
- **model** - Название модели (например: `gpt-4`, `gpt-3.5-turbo`, `claude-3-opus`)
- **temperature** - Температура генерации (0-2). Выше = более креативно
- **maxTokens** - Максимальное количество токенов в ответе (100-8000)
- **baseUrl** - Базовый URL для кастомного провайдера (опционально)
- **localUrl** - URL локального сервера для Ollama (по умолчанию: `http://localhost:11434`)
- **timeout** - Таймаут запроса в миллисекундах (5000-300000)

### Работа с локальными моделями

#### Ollama

1. Установите [Ollama](https://ollama.ai/)
2. Запустите Ollama и скачайте модель:
   ```bash
   ollama pull llama2
   # или
   ollama pull codellama
   ```
3. В настройках расширения:
   - Выберите провайдер "Ollama"
   - Укажите URL (по умолчанию: `http://localhost:11434`)
   - Введите название модели (например: `llama2`)
   - Нажмите "Проверить подключение" для проверки доступности
   - Сохраните настройки

#### LM Studio или другие локальные серверы

1. Запустите локальный сервер (LM Studio, vLLM и т.д.)
2. Убедитесь, что сервер использует OpenAI-совместимый API
3. В настройках расширения:
   - Выберите провайдер "Кастомный"
   - Укажите baseUrl (например: `http://localhost:1234/v1`)
   - Введите название модели
   - Нажмите "Проверить подключение"
   - Сохраните настройки

## Лицензия

MIT

## Вклад в проект

Приветствуются любые предложения и улучшения! Пожалуйста, создавайте Issues и Pull Requests.

